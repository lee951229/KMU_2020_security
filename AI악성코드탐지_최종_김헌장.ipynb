{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    },
    "colab": {
      "name": "AI악성코드탐지_최종_김헌장.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFGIY_bGf9IZ"
      },
      "source": [
        "#정보보호와 시스템보안\n",
        "##AI악성코드탐지 프로젝트\n",
        "> 팀이름\n",
        "+ 이헌재(20163148)\n",
        "+ 김동욱(20163090)\n",
        "+ 장한영(20171696)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-EizA023JW2"
      },
      "source": [
        "###구글드라이브 연동 및 데이터 압축해제"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fnuPpmuRC6s_",
        "outputId": "1cff6d38-9ec9-4320-a7ef-7e912c813241"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8rKVSIiC6B9"
      },
      "source": [
        "!unzip /content/drive/My\\ Drive/데이터.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmEIgngt3Tab"
      },
      "source": [
        "###라이브러리 import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjTg91Jwa8IX"
      },
      "source": [
        "import os\n",
        "import glob\n",
        "import json\n",
        "import pprint\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_selection import RFE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgtPCrwu3jWV"
      },
      "source": [
        "###학습시 사용 함수 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzMWs6xTa8IX"
      },
      "source": [
        "SEED = 41\n",
        "\n",
        "def read_label_csv(path):\n",
        "    label_table = dict()\n",
        "    with open(path, \"r\", encoding=\"cp949\") as f:\n",
        "        for line in f.readlines()[1:]:\n",
        "            fname, label = line.strip().split(\",\")\n",
        "            label_table[fname] = int(label)\n",
        "    return label_table\n",
        "\n",
        "def read_json(path):\n",
        "    with open(path, \"r\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def load_model(**kwargs):\n",
        "    if kwargs[\"model\"] == \"rf\":\n",
        "        return RandomForestClassifier(random_state=kwargs[\"random_state\"], n_jobs=4)\n",
        "    elif kwargs[\"model\"] == \"dt\":\n",
        "        return DecisionTreeClassifier(random_state=kwargs[\"random_state\"])\n",
        "    elif kwargs[\"model\"] == \"lgb\":\n",
        "        return LGBMClassifier(random_state=kwargs[\"random_state\"])\n",
        "    elif kwargs[\"model\"] == \"svm\":\n",
        "        return SVC(random_state=kwargs[\"random_state\"])\n",
        "    elif kwargs[\"model\"] == \"lr\":\n",
        "        return LogisticRegression(random_state=kwargs[\"random_state\"], n_jobs=-1)\n",
        "    elif kwargs[\"model\"] == \"knn\":\n",
        "        return KNeighborsClassifier(n_jobs=-1)\n",
        "    elif kwargs[\"model\"] == \"adaboost\":\n",
        "        return AdaBoostClassifier(random_state=kwargs[\"random_state\"])\n",
        "    elif kwargs[\"model\"] == \"mlp\":\n",
        "        return MLPClassifier(random_state=kwargs[\"random_state\"])\n",
        "    else:\n",
        "        print(\"Unsupported Algorithm\")\n",
        "        return None\n",
        "    \n",
        "\n",
        "def train(X_train, y_train, model):\n",
        "    '''\n",
        "        머신러닝 모델을 선택하여 학습을 진행하는 함수\n",
        "\t\n",
        "        :param X_train: 학습할 2차원 리스트 특징벡터\n",
        "        :param y_train: 학습할 1차원 리스트 레이블 벡터\n",
        "        :param model: 문자열, 선택할 머신러닝 알고리즘\n",
        "        :return: 학습된 머신러닝 모델 객체\n",
        "    '''\n",
        "    clf = load_model(model=model, random_state=SEED)\n",
        "    clf.fit(X_train, y_train)\n",
        "    return clf\n",
        "\n",
        "\n",
        "def evaluate(X_test, y_test, model):\n",
        "    '''\n",
        "        학습된 머신러닝 모델로 검증 데이터를 검증하는 함수\n",
        "\t\n",
        "        :param X_test: 검증할 2차원 리스트 특징 벡터\n",
        "        :param y_test: 검증할 1차원 리스트 레이블 벡터\n",
        "        :param model: 학습된 머신러닝 모델 객체\n",
        "    '''\n",
        "    predict = model.predict(X_test)\n",
        "    print(\"정확도\", model.score(X_test, y_test))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZrvIdIwa8IX"
      },
      "source": [
        "### 레이블 테이블 로드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfk2y9tCa8IX"
      },
      "source": [
        "label_table = read_label_csv(\"/content/학습데이터_정답.csv\")\n",
        "label_table2 = read_label_csv(\"/content/검증데이터_정답.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MC5nj0eoa8IX"
      },
      "source": [
        "### 특징 벡터 생성 예시\n",
        "- PEMINER 정보는 모두 수치형 데이터이므로 특별히 가공을 하지 않고 사용 가능\n",
        "- EMBER, PESTUDIO 정보는 가공해서 사용해야 할 특징들이 있음 (e.g. imports, exports 등의 문자열 정보를 가지는 데이터)\n",
        "- 수치형 데이터가 아닌 데이터(범주형 데이터)를 어떻게 가공할 지가 관건 >> 인코딩 (e.g. 원핫인코딩, 레이블인코딩 등)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2UyONtihkN9"
      },
      "source": [
        "##PEMINER 특징 추출\n",
        "---\n",
        "###peminer 특징 벡터 제거 코드\n",
        "- peminer 기존 전체 188개의 피처중 학습을 방해하는 특징 벡터를 제거하기 위함\n",
        "\n",
        "- for문을 통해 피처를 한개씩 제거하여 정확도가 오르는 특징 벡터들을 확인\n",
        "  - 결과 : [20,50,51,56,72,73,74,80,102,106,107,149,150,151,152,153,154,156,159,165,168]를 제거 하였을때 가장 높은 정확도가 나왔음 \n",
        "\n",
        "```python\n",
        "import os\n",
        "# ax에서 하나씩 빼보면서 peminer을 돌려서 정확도가 올라가는걸 찾음\n",
        "ax=[20,50,51,56,72,73,74,80,102,106,107,149,150,151,152,153,154,156,159,165,168]\n",
        "\n",
        "\n",
        "am=0.9551\n",
        "path_dir = f\"/content/PEMINER/학습데이터\"\n",
        "\n",
        "path_dir2 = f\"/content/PEMINER/검증데이터\"\n",
        "file_list = os.listdir(path_dir)\n",
        "\n",
        "file_list2 = os.listdir(path_dir2)\n",
        "\n",
        "for s in range(len(ax)):\n",
        "  ass= ax[:]\n",
        "  del ass[s]\n",
        "  X, y = [], []\n",
        "  X_test, y_test = [], []\n",
        "  for i in range(len(file_list)):\n",
        "    feature_vector = []\n",
        "    for data in [\"PEMINER\"]:\n",
        "        path = f\"/content/{data}/학습데이터/{file_list[i]}\"\n",
        "        label = label_table[file_list[i].split('.')[0]]\n",
        "        if data == \"PEMINER\": \n",
        "            feature_vector += PeminerParser(path).process_report(ass)\n",
        "    X.append(feature_vector)\n",
        "    y.append(label)\n",
        "  for i in range(len(file_list2)):\n",
        "    feature_vector = []\n",
        "    for data in [\"PEMINER\"]:\n",
        "        path = f\"/content/{data}/검증데이터/{file_list2[i]}\"\n",
        "        label = label_table2[file_list2[i].split('.')[0]]\n",
        "        if data == \"PEMINER\": \n",
        "            feature_vector += PeminerParser(path).process_report(ass)\n",
        "    X_test.append(feature_vector)\n",
        "    y_test.append(label)\n",
        " \n",
        "  models = []\n",
        "  for model in [\"rf\", \"lgb\"]:\n",
        "      clf = train(X, y, model)\n",
        "      models.append(clf)\n",
        "\n",
        "  for model in models:\n",
        "      evaluate(X_test, y_test, model)\n",
        "  def ensemble_result(X, y, models):\n",
        "    \n",
        "    predicts = []\n",
        "    for model in models:\n",
        "        prob = [result for _, result in model.predict_proba(X)]\n",
        "        predicts.append(prob)\n",
        "    \n",
        "    predict = np.mean(predicts, axis=0)\n",
        "    predict = [1 if x >= 0.5 else 0 for x in predict]\n",
        "        \n",
        "    global am\n",
        "    if accuracy_score(y, predict)>am:\n",
        "     \n",
        "      \n",
        "\n",
        "      am=accuracy_score(y, predict)\n",
        "      print(\"ax정확도\",s, accuracy_score(y, predict))\n",
        "  ensemble_result(X_test, y_test, models)\n",
        "print(ax)\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBIwwg6ohaYu"
      },
      "source": [
        "class PeminerParser:\n",
        "    def __init__(self, path):\n",
        "        self.report = read_json(path)\n",
        "        self.vector = []\n",
        "    \n",
        "    def process_report(self):\n",
        "\n",
        "        a=[20,50,51,56,72,73,80,102,106,107,149,150,151,152,153,154,156,159,165,168]\n",
        "        vector3 = [value for _, value in sorted(self.report.items(), key=lambda x: x[0])]\n",
        "\n",
        "        for i in range(len(a)):\n",
        "          del vector3[a[i]-i]\n",
        "\n",
        "        self.vector = vector3\n",
        "\n",
        "        return self.vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryK9Fr1Gkpty"
      },
      "source": [
        "### EMBER 특징 추출\n",
        "---\n",
        "- 기존에 있던 특징들을 수정한다.\n",
        " > 'general'과 'strings'에 있던 특징들을 하나씩 제거해보며 정확도가 증가하면 제거, 감소하면 다시 특징으로 추출한다.-> 결국 기존에 있던 특징 제거\n",
        "\n",
        "- 새로운 특징을 찾아 vector에 하나씩 추가하여 정확도가 증가하면 추가, 감소하면 제거한다.\n",
        " - 증가한 특징\n",
        "   > get_entropy_max(), get_bigentropy_len(), numimport(), get_byteentropy_max(), get_histogram_max(), get_datadirsize_max() ,numexport(), get_datadirnum(), get_sizeof_code(), get_byteennum()\n",
        " - 감소한 특징\n",
        "   > get_general_file_info(), get_histogram_info(), get_string_info(), get_sectionsize_max(), get_sectionvsize_max(), get_byteentropy_info(), et_max_entropy(), get_string_max()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UivWM1f_hey3"
      },
      "source": [
        "class EmberParser:\n",
        "    '''\n",
        "        예제에서 사용하지 않은 특징도 사용하여 벡터화 할 것을 권장\n",
        "    '''\n",
        "    def __init__(self, path):\n",
        "        self.report = read_json(path)\n",
        "        self.vector = []\n",
        "    \n",
        "    def get_histogram_info(self):\n",
        "        histogram = np.array(self.report[\"histogram\"])\n",
        "        total = histogram.sum()\n",
        "        vector = histogram / total\n",
        "        return vector.tolist()\n",
        "    \n",
        "    def get_string_info(self):\n",
        "        strings = self.report[\"strings\"]\n",
        "\n",
        "        hist_divisor = float(strings['printables']) if strings['printables'] > 0 else 1.0\n",
        "        vector = [\n",
        "            strings['numstrings'], \n",
        "            strings['avlength'], \n",
        "            strings['printables'],\n",
        "            strings['entropy'], \n",
        "            #strings['paths'], \n",
        "            #strings['urls'],\n",
        "            strings['registry'], \n",
        "            strings['MZ']\n",
        "        ]\n",
        "        vector += (np.asarray(strings['printabledist']) / hist_divisor).tolist()\n",
        "        return vector\n",
        "    \n",
        "    def get_general_file_info(self):\n",
        "        general = self.report[\"general\"]\n",
        "        vector = [\n",
        "            #general['size'], \n",
        "            general['vsize'], \n",
        "            general['has_debug'], \n",
        "            general['exports'], \n",
        "            general['imports'],\n",
        "            general['has_relocations'], \n",
        "            general['has_resources'], \n",
        "            general['has_signature'], \n",
        "            general['has_tls'],\n",
        "            #general['symbols']\n",
        "        ]\n",
        "        return vector\n",
        "    # 특징 추가\n",
        "    def get_byteentropy_info(self):\n",
        "        byteentropy = np.array(self.report[\"byteentropy\"])\n",
        "        total = byteentropy.sum()\n",
        "        vector = byteentropy / total\n",
        "        return vector.tolist()\n",
        "        \n",
        "    def get_byteentropy_max(self):\n",
        "        byteentropy = np.array(self.report[\"byteentropy\"])\n",
        "        maxi = np.max(byteentropy)\n",
        "        vector = [maxi]\n",
        "        return vector\n",
        "\n",
        "    def get_histogram_max(self):\n",
        "        histogram = np.array(self.report[\"histogram\"])\n",
        "        maxi = np.max(histogram)\n",
        "        vector = [maxi]\n",
        "        return vector\n",
        "\n",
        "    def get_string_max(self):\n",
        "        string_max = np.array(self.report[\"strings\"]['printabledist'])\n",
        "        maxi = np.max(string_max)\n",
        "        vector = [maxi]\n",
        "        return vector\n",
        "\n",
        "    def get_datadirsize_max(self):\n",
        "        data = np.array(self.report[\"datadirectories\"])\n",
        "        aaaa=[-1]\n",
        "        for i in range(len(data)):\n",
        "          aaaa.append(data[i][\"size\"])\n",
        "        vector = [max(aaaa)]\n",
        "        return vector\n",
        "\n",
        "    def get_sectionsize_max(self):\n",
        "        section = np.array(self.report[\"section\"][\"sections\"])\n",
        "        aaaa=[999999999]\n",
        "        for i in range(len(section)):\n",
        "          aaaa.append(section[i][\"size\"])\n",
        "        vector = [min(aaaa)]\n",
        "        return vector\n",
        "\n",
        "    def get_sectionvsize_max(self):\n",
        "        section = np.array(self.report[\"section\"][\"sections\"])\n",
        "        aaaa=[999999999]\n",
        "        for i in range(len(section)):\n",
        "          aaaa.append(section[i][\"vsize\"])\n",
        "        vector = [min(aaaa)]\n",
        "        return vector\n",
        "\n",
        "    def get_entropy_max(self):\n",
        "        section = np.array(self.report[\"section\"][\"sections\"])\n",
        "        aaaa=[999999999]\n",
        "        for i in range(len(section)):\n",
        "          aaaa.append(int(section[i][\"entropy\"]))\n",
        "        vector = [max(aaaa)]\n",
        "        return vector\n",
        "\n",
        "    def get_bigentropy_len(self):\n",
        "        section = np.array(self.report[\"section\"][\"sections\"])\n",
        "        aaaa=0\n",
        "        for i in range(len(section)):\n",
        "          if section[i][\"entropy\"]>=6:\n",
        "            aaaa+=1 \n",
        "        vector = [aaaa]\n",
        "        return vector\n",
        "\n",
        "    def numimport(self):\n",
        "        import1 = self.report[\"imports\"]\n",
        "        try:\n",
        "          vector = [len(import1)]\n",
        "        except:\n",
        "          vector =[-1] \n",
        "        return vector\n",
        "\n",
        "    def numexport(self):\n",
        "        export1 = self.report[\"exports\"]\n",
        "        try:\n",
        "          vector = [len(export1)]\n",
        "        except:\n",
        "          vector =[-1] \n",
        "        return vector\n",
        "\n",
        "    def get_datadirnum(self):\n",
        "        data = np.array(self.report[\"datadirectories\"])\n",
        "        vector = [len(data)]\n",
        "        return vector\n",
        "\n",
        "    def get_sizeof_code(self):\n",
        "        sizeofcode = np.array(self.report[\"header\"][\"optional\"][\"sizeof_code\"])\n",
        "  \n",
        "        vector = [float(sizeofcode)]\n",
        "        return vector\n",
        "\n",
        "    def get_byteennum(self):\n",
        "        data = np.array(self.report[\"byteentropy\"])\n",
        "        vector = [len(data)]\n",
        "        return vector\n",
        "\n",
        "    def process_report(self):\n",
        "        vector = []\n",
        "        #vector += self.get_general_file_info()\n",
        "        #vector += self.get_histogram_info()\n",
        "        #vector += self.get_string_info()    \n",
        "        #vector += self.get_sectionsize_max()\n",
        "        #vector += self.get_sectionvsize_max()\n",
        "        #vector += self.get_byteentropy_info()\n",
        "        #vector += self.get_max_entropy()\n",
        "        #vector += self.get_string_max()     \n",
        "\n",
        "        vector += self.get_entropy_max()\n",
        "        vector += self.get_bigentropy_len()\n",
        "        vector += self.numimport()       \n",
        "        vector += self.get_byteentropy_max()\n",
        "        vector += self.get_histogram_max()        \n",
        "        vector += self.get_datadirsize_max()\n",
        "        vector += self.numexport()\n",
        "        vector += self.get_datadirnum()\n",
        "        vector += self.get_sizeof_code()\n",
        "        vector += self.get_byteennum()\n",
        "        '''\n",
        "            특징 추가\n",
        "        '''\n",
        "        return vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyaRMryZhpEJ"
      },
      "source": [
        "### PESTUDIO 특징 추출\n",
        "---\n",
        "- 새로운 특징을 찾아 vector에 하나씩 추가하여 정확도가 증가하면 추가, 감소하면 제거한다.\n",
        " - 증가한 특징\n",
        "   > get_entropy_max(), get_bigentropy_len(), numimport(), get_byteentropy_max(), get_histogram_max(), get_datadirsize_max(), numexport(), get_datadirnum(), get_sizeof_code(), get_byteennum()\n",
        " - 감소한 특징\n",
        "   > get_general_file_info(), get_histogram_info(), get_string_info(), get_sectionsize_max(), get_sectionvsize_max(), get_byteentropy_info(), get_max_entropy(), get_string_max()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIygldKDa8IX"
      },
      "source": [
        "class PestudioParser:\n",
        "    '''\n",
        "        사용할 특징을 선택하여 벡터화 할 것을 권장\n",
        "    '''\n",
        "    def get_indicators_info(self):\n",
        "        count = 0;\n",
        "        try:\n",
        "          image = self.report[\"image\"][\"indicators\"][\"indicator\"]\n",
        "          for i in range(0, len(image)):\n",
        "            if(image[i][\"@severity\"] == \"1\" or image[i][\"@severity\"] == \"2\"):\n",
        "              count+=1\n",
        "        except:\n",
        "          count = -1\n",
        "        vector = [count]\n",
        "        return vector\n",
        "\n",
        "    def numsection(self):\n",
        "        try:\n",
        "          section = self.report[\"image\"][\"sections\"][\"section\"]\n",
        "          vector = [(len(section))]\n",
        "        except:\n",
        "          vector =[-1] \n",
        "        return vector\n",
        "\n",
        "    def blacklist(self):\n",
        "        try:\n",
        "          section = self.report[\"image\"][\"sections\"][\"section\"]\n",
        "          count = 0\n",
        "          for i in range(len(section)):\n",
        "            if section[i][\"@blacklisted\"] == \"x\":\n",
        "                count += 1\n",
        "        except:\n",
        "          count =-1\n",
        "        vector = [count]\n",
        "        return vector\n",
        "\n",
        "    def import_blacklist(self):\n",
        "        try:\n",
        "          imports = self.report[\"image\"][\"imports\"][\"import\"]\n",
        "          count = 0\n",
        "          for i in range(len(imports)):\n",
        "              if imports[i][\"@blacklist\"] == \"x\":\n",
        "                count += 1\n",
        "        except:\n",
        "          count = -1\n",
        "        vector = [count]\n",
        "        return vector\n",
        "\n",
        "    def libraries_blacklist(self):\n",
        "        try:\n",
        "          libraries = self.report[\"image\"][\"libraries\"][\"library\"]\n",
        "          count = 0\n",
        "          for i in range(len(libraries)):\n",
        "              if libraries[i][\"@blacklist\"] == \"x\":\n",
        "                count += 1\n",
        "        except:\n",
        "          count = -1\n",
        "        vector = [count]\n",
        "        return vector\n",
        "\n",
        "    def overview_entropy(self):\n",
        "        try:\n",
        "          entropy = self.report[\"image\"][\"overview\"][\"entropy\"]\n",
        "          vector = [float(entropy)]\n",
        "        except:\n",
        "          vector = [-1]\n",
        "        return vector\n",
        "\n",
        "    def size(self):\n",
        "        try:\n",
        "          vec=[]\n",
        "          instance = self.report[\"image\"][\"resources\"][\"instance\"]\n",
        "          for i in range(len(instance)):\n",
        "            vec.append(int(instance[i]['@size']))\n",
        "          vector = [max(vec)]\n",
        "        except:\n",
        "          vector = [-1]\n",
        "        return vector     \n",
        "\n",
        "    def entropy(self):\n",
        "        try:\n",
        "          vec=[]\n",
        "          instance = self.report[\"image\"][\"resources\"][\"instance\"]\n",
        "          for i in range(len(instance)):\n",
        "            vec.append(int(instance[i]['@entropy']))\n",
        "          vector = [max(vec)]\n",
        "        except:\n",
        "          vector = [-1]\n",
        "        return vector   \n",
        "\n",
        "    def string_size_max(self):\n",
        "        try:\n",
        "          vec=[]\n",
        "          string = self.report[\"image\"][\"strings\"][\"string\"]\n",
        "          for i in range(len(string)):\n",
        "            vec.append(int(string[i]['@size']))\n",
        "          vector = [max(vec)]\n",
        "        except:\n",
        "          vector = [-1]\n",
        "        return vector  \n",
        "\n",
        "    def get_debug(self):\n",
        "        try:\n",
        "          image = self.report[\"debug\"]\n",
        "          if image['debug'] == 'n/a' :\n",
        "            vector = [0]\n",
        "          else :\n",
        "            vector = [1]  \n",
        "        except :\n",
        "            vector = [-1]      \n",
        "        return vector\n",
        "\n",
        "    def get_string_leng(self):\n",
        "        try:\n",
        "          image = self.report[\"image\"]\n",
        "          vector = [ len(image['strings']['ascii']['string']) ]\n",
        "        except:\n",
        "          vector = [-1]\n",
        "        return vector\n",
        "\n",
        "    def get_tls_callbacks(self):\n",
        "        try:\n",
        "          image = self.report[\"image\"]\n",
        "          if image['tls-callbacks'] == 'n/a' :\n",
        "            vector = [0]\n",
        "          else :\n",
        "            vector = [1]\n",
        "        except :\n",
        "          vector = [-1]\n",
        "        return vector\n",
        "\n",
        "    def get_certificate(self):\n",
        "        try:\n",
        "          image = self.report[\"image\"]\n",
        "          if image['certificate'] == 'n/a' :\n",
        "            vector = [0]\n",
        "          else :\n",
        "            vector = [1]\n",
        "        except :\n",
        "          vector = [-1]\n",
        "        return vector\n",
        "\n",
        "    def get_overlay(self):\n",
        "        try : \n",
        "          image = self.report[\"image\"]\n",
        "          if image['overlay'] == 'n/a' :\n",
        "            vector = [0]\n",
        "          else :\n",
        "            vector = [1]      \n",
        "        except:\n",
        "          vector = [-1]\n",
        "        return vector   \n",
        "\n",
        "    def __init__(self, path):\n",
        "        try:\n",
        "          self.report = read_json(path)\n",
        "        except:\n",
        "          self.report = None\n",
        "        self.vector = []\n",
        "\n",
        "    def process_report(self):\n",
        "        vector = []\n",
        "\n",
        "        #vector += self.get_debug()\n",
        "        #vector += self.get_string_leng()\n",
        "        #vector += self.get_tls_callbacks()\n",
        "        #vector += self.get_certificate()\n",
        "        #vector += self.get_overlay()\n",
        "        #vector += self.size()\n",
        "\n",
        "        vector += self.string_size_max()\n",
        "        vector += self.entropy()\n",
        "        vector += self.get_indicators_info()\n",
        "        vector += self.numsection()\n",
        "        vector += self.blacklist()\n",
        "        vector += self.import_blacklist()\n",
        "        vector += self.libraries_blacklist()\n",
        "        vector += self.overview_entropy()\n",
        "\n",
        "        return vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JkbYmZMHMzj"
      },
      "source": [
        "#### 특징 추가, 삭제, 변경에 따른 정확도 기록\n",
        "https://docs.google.com/document/d/1K17iDGk5jYcMP_y0-LQnURWK7TYkvGIzRR5FNPrpWhw/edit?usp=sharing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvxgGEJpa8IX"
      },
      "source": [
        "### 학습데이터 구성\n",
        " - 데이터의 특징 벡터 모음(2차원 리스트) : X\n",
        " - 데이터의 레이블 모음(1차원 리스트) : y\n",
        " - 이부분에서 모든 파일을 읽어와야 한다 엠버와 피마이너 파일의 학습데이터 안에 들어있는\n",
        " - **주의사항**\n",
        "   1. 특징 벡터 구성은 2차원이여야 한다.\n",
        "   2. 각 벡터는 1차원 리스트, 벡터 크기는 모두 같아야함"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pwI8Ba1pa8IX",
        "outputId": "a7375753-eb2c-407b-ad44-c51ece357652"
      },
      "source": [
        "X, y = [], []\n",
        "\n",
        "path_dir = f\"/content/PEMINER/학습데이터\"\n",
        "\n",
        "file_list = os.listdir(path_dir)\n",
        "\n",
        "for i in range(len(file_list)):\n",
        "    feature_vector = []\n",
        "    for data in [\"PEMINER\",\"EMBER\",\"PESTUDIO\"]:\n",
        "        path = f\"/content/{data}/학습데이터/{file_list[i]}\"\n",
        "        label = label_table[file_list[i].split('.')[0]]\n",
        "        if data == \"PEMINER\": \n",
        "            feature_vector += PeminerParser(path).process_report()\n",
        "        elif data == \"EMBER\":\n",
        "            feature_vector += EmberParser(path).process_report()\n",
        "        elif data == \"PESTUDIO\":\n",
        "            feature_vector += PestudioParser(path).process_report()\n",
        "            \n",
        "    X.append(feature_vector)\n",
        "    y.append(label)\n",
        "\n",
        "print(np.asarray(X).shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(20000, 186)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtGr_Lrs5Aon"
      },
      "source": [
        "###검증데이터 구성\n",
        "\n",
        " - 데이터의 특징 벡터 모음(2차원 리스트) : X_test\n",
        " - 데이터의 레이블 모음(1차원 리스트) : y_test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UgqL3hTqejI",
        "outputId": "3917e107-b512-466d-afbe-db84b2c72599"
      },
      "source": [
        "X_test, y_test = [], []\n",
        "\n",
        "path_dir = f\"/content/PEMINER/검증데이터\"\n",
        "\n",
        "file_list = os.listdir(path_dir)\n",
        "\n",
        "for i in range(len(file_list)):\n",
        "    feature_vector = []\n",
        "    for data in [\"PEMINER\",\"EMBER\",\"PESTUDIO\"]:\n",
        "        path = f\"/content/{data}/검증데이터/{file_list[i]}\"\n",
        "        label = label_table2[file_list[i].split('.')[0]]\n",
        "        if data == \"PEMINER\": \n",
        "            feature_vector += PeminerParser(path).process_report()\n",
        "        elif data == \"EMBER\":\n",
        "            feature_vector += EmberParser(path).process_report()\n",
        "        elif data == \"PESTUDIO\":\n",
        "            feature_vector += PestudioParser(path).process_report()\n",
        "    \n",
        "    X_test.append(feature_vector)\n",
        "    y_test.append(label)\n",
        "    \n",
        "print(np.asarray(X_test).shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10000, 186)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ae4Z1Wdt7O0M"
      },
      "source": [
        "###학습데이터(2만개)+검증데이터(1만개) 모두 학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CxgqRcJLHF86",
        "outputId": "4f034246-5c3e-472b-b26e-ee089ef343a5"
      },
      "source": [
        "X, y = [], []\n",
        "\n",
        "path_dir = f\"/content/PEMINER/학습데이터\"\n",
        "\n",
        "file_list = os.listdir(path_dir)\n",
        "\n",
        "for i in range(len(file_list)):\n",
        "    feature_vector = []\n",
        "    for data in [\"PEMINER\",\"EMBER\",\"PESTUDIO\"]:\n",
        "        path = f\"/content/{data}/학습데이터/{file_list[i]}\"\n",
        "        label = label_table[file_list[i].split('.')[0]]\n",
        "        if data == \"PEMINER\": \n",
        "            feature_vector += PeminerParser(path).process_report()\n",
        "        elif data == \"EMBER\":\n",
        "            feature_vector += EmberParser(path).process_report()\n",
        "        elif data == \"PESTUDIO\":\n",
        "            feature_vector += PestudioParser(path).process_report()\n",
        "            \n",
        "    X.append(feature_vector)\n",
        "    y.append(label)\n",
        "\n",
        "print(np.asarray(X).shape)\n",
        "\n",
        "path_dir = f\"/content/PEMINER/검증데이터\"\n",
        "\n",
        "file_list = os.listdir(path_dir)\n",
        "\n",
        "for i in range(len(file_list)):\n",
        "    feature_vector = []\n",
        "    for data in [\"PEMINER\",\"EMBER\",\"PESTUDIO\"]:\n",
        "        path = f\"/content/{data}/검증데이터/{file_list[i]}\"\n",
        "        label = label_table2[file_list[i].split('.')[0]]\n",
        "        if data == \"PEMINER\": \n",
        "            feature_vector += PeminerParser(path).process_report()\n",
        "        elif data == \"EMBER\":\n",
        "            feature_vector += EmberParser(path).process_report()\n",
        "        elif data == \"PESTUDIO\":\n",
        "            feature_vector += PestudioParser(path).process_report()\n",
        "    \n",
        "    X.append(feature_vector)\n",
        "    y.append(label)\n",
        "    \n",
        "print(np.asarray(X).shape, np.asarray(y).shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(20000, 186)\n",
            "(30000, 186) (30000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NO9ovHvH7OSN"
      },
      "source": [
        "models = []\n",
        "for model in [\"rf\", \"lgb\"]:\n",
        "    clf = train(X, y, model)\n",
        "    models.append(clf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wyLc7-Ua8IY"
      },
      "source": [
        "### 학습 및 검증(최종 예측 결과에는 검증이 쓰이지 않음)\n",
        "---\n",
        "알고리즘의 종류와 개수를 다양하게 앙상블 한다.\n",
        "  - algo2는 서로 다른 2개의 알고리즘을 갖는 2차원 리스트다.\n",
        "  - 마찬가지로 algo3, algo4 .... 점차 개수를 늘려 다양하게 넣어 앙상블 한다.\n",
        "  - 이 중 최고의 정확도를 가지는 알고리즘 algoMaxName과 정확도 accuracyMax를 취한다.\n",
        "    - *** Max accuracy:  ['rf', 'lgb'] 0.9607 ***\n",
        "\n",
        "```python\n",
        "algorithm = [\"rf\", \"lgb\",\"dt\",\"lr\", 'svm', 'knn', 'adaboost', 'mlp']\n",
        "algo2 = []\n",
        "for i in range(len(algorithm))):\n",
        "  for j in range(i+1,len(algorithm)): \n",
        "    algo2.append( [ algorithm[i], algorithm[j] ] )\n",
        "\n",
        "accuracyMax = 0\n",
        "algoMaxName = []\n",
        "\n",
        "for algotmp in algo2 :\n",
        "  models = []\n",
        "  try:\n",
        "    for model in algotmp:\n",
        "      clf = train(X, y, model)\n",
        "      models.append(clf)\n",
        "    \n",
        "    accu = ensemble_result_test(X_test, y_test, models)   \n",
        "    \n",
        "    if accu > accuracyMax :\n",
        "        accuracyMax = accu\n",
        "        algoMaxName = algotmp\n",
        "        print(algotmp, accu)\n",
        "    elif accu == -1 :\n",
        "        print(algotmp, \"함수 에러\")  \n",
        "    else :\n",
        "        print(algotmp, accu)\n",
        "  except :\n",
        "    print(algotmp, \"알고리즘 에러\")\n",
        "\n",
        "print(\"Max accuracy: \", algoMaxName, accuracyMax)\n",
        "\n",
        "def ensemble_result_test(X, y, models):\n",
        "    try: \n",
        "      predicts = []\n",
        "      for model in models:\n",
        "          prob = [result for _, result in model.predict_proba(X)]\n",
        "          predicts.append(prob)\n",
        "    \n",
        "      predict = np.mean(predicts, axis=0)\n",
        "      predict = [1 if x >= 0.5 else 0 for x in predict]\n",
        "      return accuracy_score(y, predict)\n",
        "\n",
        "    except :\n",
        "      return -1\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cEEMxLbma8IY",
        "outputId": "92965726-5bdf-458c-ca0a-624729684ee0"
      },
      "source": [
        "# 학습 피마이너랑 엠버를 학습시켜서 학습된 특징벡터들을 models에 넣는다 \n",
        "models = []\n",
        "for model in [\"rf\", \"lgb\"]:\n",
        "    clf = train(X, y, model)\n",
        "    models.append(clf)\n",
        "\n",
        "# 검증 피마이너랑 엠버를 각각 검증 \n",
        "# 실제 검증 시에는 제공한 검증데이터를 검증에 사용해야 함\n",
        "for model in models:\n",
        "    evaluate(X_test, y_test, model)\n",
        "\n",
        "def ensemble_result(X, y, models):\n",
        "    '''\n",
        "        학습된 모델들의 결과를 앙상블하는 함수\n",
        "   \n",
        "        :param X: 검증할 2차원 리스트 특징 벡터\n",
        "        :param y: 검증할 1차원 리스트 레이블 벡터\n",
        "        :param models: 1개 이상의 학습된 머신러닝 모델 객체를 가지는 1차원 리스트\n",
        "    '''\n",
        "    predicts = []\n",
        "    for model in models:\n",
        "        prob = [result for _, result in model.predict_proba(X)]\n",
        "        predicts.append(prob)\n",
        "    \n",
        "    predict = np.mean(predicts, axis=0)\n",
        "    predict = [1 if x >= 0.5 else 0 for x in predict]\n",
        "        \n",
        "    print(\"앙상블 후 정확도\", accuracy_score(y, predict))\n",
        "\n",
        "ensemble_result(X_test, y_test, models)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "정확도 0.9557\n",
            "정확도 0.9538\n",
            "앙상블 후 정확도 0.9607\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrLJWu9IPLgW"
      },
      "source": [
        "###테스트 데이터 구성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-o2oCVv1QOJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db25e937-383d-42ca-d70e-996c6825a66b"
      },
      "source": [
        "final_test = []\n",
        "labeltable_test = []\n",
        "\n",
        "path_dir = f\"/content/PEMINER/테스트데이터\"\n",
        "\n",
        "file_list = os.listdir(path_dir)\n",
        "\n",
        "for i in range(len(file_list)):\n",
        "    feature_vector = []\n",
        "    for data in [\"PEMINER\",\"EMBER\",\"PESTUDIO\"]:\n",
        "        path = f\"/content/{data}/테스트데이터/{file_list[i]}\"\n",
        "        label = file_list[i].split('.')[0]\n",
        "        if data == \"PEMINER\": \n",
        "            feature_vector += PeminerParser(path).process_report()\n",
        "        elif data == \"EMBER\":\n",
        "            feature_vector += EmberParser(path).process_report()\n",
        "        elif data == \"PESTUDIO\":\n",
        "            feature_vector += PestudioParser(path).process_report()\n",
        "    final_test.append(feature_vector)\n",
        "    labeltable_test.append(label)\n",
        "\n",
        "print(np.asarray(final_test).shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10000, 186)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "if3PITTt5OP4"
      },
      "source": [
        "### 테스트 데이터 학습 및 앙상블\n",
        "테스트 데이터는 결과가 없기 때문에 y 데이터가 없다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyTJw2TBQFjJ"
      },
      "source": [
        "def ensemble_data(X, models):\n",
        "    '''\n",
        "        학습된 모델들의 결과를 앙상블하는 함수\n",
        "          \n",
        "        :param X: 검증할 2차원 리스트 특징 벡터\n",
        "        :param models: 1개 이상의 학습된 머신러닝 모델 객체를 가지는 1차원 리스트\n",
        "    '''\n",
        "\n",
        "    predicts = []\n",
        "    for model in models:\n",
        "        prob = [result for _, result in model.predict_proba(X)]\n",
        "        predicts.append(prob)\n",
        "    \n",
        "    predict = np.mean(predicts, axis=0)\n",
        "    predict = [ 1 if x >= 0.5 else 0 for x in predict ]\n",
        "        \n",
        "    return predict\n",
        "  \n",
        "result = ensemble_data(final_test, models)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ejv1rTL75RUq"
      },
      "source": [
        "###테스트 데이터 예측.csv 파일 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vAmVL8GRTrn"
      },
      "source": [
        "import csv\n",
        "\n",
        "predict_test = []\n",
        "\n",
        "for i in range(len(labeltable_test)):\n",
        "    predict_test.append([labeltable_test[i], result[i]])\n",
        "\n",
        "path = \"/content/drive/MyDrive/predict.csv\"\n",
        "\n",
        "with open(path, 'w', newline='') as f:\n",
        "    w = csv.writer(f)\n",
        "    w.writerow(['file', 'predict'])\n",
        "\n",
        "    for i in predict_test :\n",
        "        w.writerow(i)\n",
        "        \n",
        "    f.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}